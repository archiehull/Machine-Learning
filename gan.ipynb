{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 13:20:20.637906: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import keras as ks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path='./triple_mnist/'\n",
    "\n",
    "def flatten_toarray(set):\n",
    "    set_path=db_path+set\n",
    "    sub_folders=os.listdir(set_path)\n",
    "\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "\n",
    "    for folder in sub_folders:\n",
    "        subfolder_path = os.path.join(set_path, folder)\n",
    "\n",
    "        for img_file in os.listdir(subfolder_path):\n",
    "            img_path = os.path.join(subfolder_path, img_file)\n",
    "\n",
    "            flattened_img = (Image.open(img_path))\n",
    "            flattened_img = (np.array((flattened_img).convert('L')))\n",
    "\n",
    "            imgs.append(flattened_img)\n",
    "            labels.append(int(folder))\n",
    "            \n",
    "    return np.array(imgs), np.array(labels)\n",
    "\n",
    "\n",
    "f_train_imgs, f_train_labels = flatten_toarray('train')\n",
    "f_test_imgs, f_test_labels = flatten_toarray('test')\n",
    "f_val_imgs, f_val_labels = flatten_toarray('val')\n",
    "\n",
    "f_train_imgs = f_train_imgs / 255.0\n",
    "f_test_imgs = f_test_imgs / 255.0\n",
    "f_val_imgs = f_val_imgs / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path='./triple_mnist/'\n",
    "\n",
    "def splits_flatten_toarray(set):\n",
    "    set_path=db_path+set\n",
    "    sub_folders=os.listdir(set_path)\n",
    "\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "\n",
    "    for folder in sub_folders:\n",
    "        subfolder_path = os.path.join(set_path, folder)\n",
    "\n",
    "        for img_file in os.listdir(subfolder_path):\n",
    "            img_path = os.path.join(subfolder_path, img_file)\n",
    "\n",
    "            img = Image.open(img_path).convert('L').resize((45,45))\n",
    "            \n",
    "            width, height = img.size\n",
    "            \n",
    "            split_width = width // 3\n",
    "\n",
    "            splits = [\n",
    "                img.crop((i * split_width, 0, (i + 1) * split_width, height))\n",
    "                for i in range(3)\n",
    "            ]\n",
    "\n",
    "            for i, split in enumerate(splits):\n",
    "                imgs.append(np.array(split))  \n",
    "                labels.append(int(folder[i]))\n",
    "\n",
    "    return np.array(imgs), np.array(labels)\n",
    "\n",
    "\n",
    "t_train_imgs, t_train_labels = splits_flatten_toarray('train')\n",
    "t_test_imgs, t_test_labels = splits_flatten_toarray('test')\n",
    "t_val_imgs, t_val_labels = splits_flatten_toarray('val')\n",
    "\n",
    "t_train_imgs = t_train_imgs / 255.0\n",
    "t_test_imgs = t_test_imgs / 255.0\n",
    "t_val_imgs = t_val_imgs / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path='./triple_mnist/'\n",
    "\n",
    "def splits_flatten_toarray(set):\n",
    "    set_path=db_path+set\n",
    "    sub_folders=os.listdir(set_path)\n",
    "\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "\n",
    "    for folder in sub_folders:\n",
    "        subfolder_path = os.path.join(set_path, folder)\n",
    "\n",
    "        for img_file in os.listdir(subfolder_path):\n",
    "            img_path = os.path.join(subfolder_path, img_file)\n",
    "\n",
    "            img = Image.open(img_path).convert('L')\n",
    "            \n",
    "            width, height = img.size\n",
    "            \n",
    "            split_width = width // 3\n",
    "\n",
    "            splits = [\n",
    "                img.crop((i * split_width, 0, (i + 1) * split_width, height))\n",
    "                for i in range(3)\n",
    "            ]\n",
    "\n",
    "            for i, split in enumerate(splits):\n",
    "                imgs.append(np.array(split))  \n",
    "                labels.append(int(folder[i]))\n",
    "\n",
    "    return np.array(imgs), np.array(labels)\n",
    "\n",
    "\n",
    "ft_train_imgs, ft_train_labels = splits_flatten_toarray('train')\n",
    "ft_test_imgs, ft_test_labels = splits_flatten_toarray('test')\n",
    "ft_val_imgs, ft_val_labels = splits_flatten_toarray('val')\n",
    "\n",
    "ft_train_imgs = ft_train_imgs / 255.0\n",
    "ft_test_imgs = ft_test_imgs / 255.0\n",
    "ft_val_imgs = ft_val_imgs / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64000, 3)\n",
      "(64000, 84, 84, 1)\n"
     ]
    }
   ],
   "source": [
    "rs_t_train_labels = t_train_labels.reshape(64000, 3)\n",
    "rs_f_train_imgs = tf.convert_to_tensor(f_train_imgs)\n",
    "rs_f_train_imgs = tf.reshape(rs_f_train_imgs, (64000, 84, 84, 1))\n",
    "\n",
    "\n",
    "print(rs_t_train_labels.shape)\n",
    "print(rs_f_train_imgs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    # Label embedding\n",
    "    label_embedding = layers.Embedding(10, 16, name=\"label_embedding\")\n",
    "\n",
    "    # Label projection\n",
    "    label_proj = layers.Dense(128 * 6 * 6, name=\"label_proj\")\n",
    "\n",
    "    # Noise projection\n",
    "    noise_proj = layers.Dense(256 * 6 * 6, name=\"noise_proj\")\n",
    "\n",
    "    # Convolutional blocks remain the same\n",
    "    conv_blocks = tf.keras.Sequential([\n",
    "        layers.Conv2DTranspose(256, kernel_size=2, strides=7, padding='same', use_bias=False, \n",
    "                             input_shape=(384, 6, 6), data_format='channels_first'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', \n",
    "                             use_bias=False, data_format='channels_first'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        layers.Conv2DTranspose(1, kernel_size=4, strides=1, padding='same', \n",
    "                             use_bias=False, data_format='channels_first'),\n",
    "        layers.Activation('tanh')\n",
    "    ])\n",
    "\n",
    "    def generator(noise, labels):\n",
    "        # Handle dynamic batch size using tf.shape\n",
    "        batch_size = tf.shape(noise)[0]\n",
    "        \n",
    "        # Use tf.slice instead of Python slicing\n",
    "        labels = tf.slice(labels, [0, 0], [batch_size, -1])\n",
    "        \n",
    "        # Embed labels\n",
    "        emb1 = label_embedding(labels[:, 0])\n",
    "        emb2 = label_embedding(labels[:, 1])\n",
    "        emb3 = label_embedding(labels[:, 2])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        emb_cat = tf.concat([emb1, emb2, emb3], axis=1)\n",
    "        \n",
    "        # Project labels\n",
    "        label_feat = label_proj(emb_cat)\n",
    "        label_feat = tf.reshape(label_feat, tf.concat([[batch_size], [128, 6, 6]], axis=0))\n",
    "        \n",
    "        # Project noise\n",
    "        noise_feat = noise_proj(noise)\n",
    "        noise_feat = tf.reshape(noise_feat, tf.concat([[batch_size], [256, 6, 6]], axis=0))\n",
    "        \n",
    "        # Combine features\n",
    "        combined = tf.concat([noise_feat, label_feat], axis=1)\n",
    "        \n",
    "        # Generate output\n",
    "        output = conv_blocks(combined)\n",
    "        output = tf.reshape(output, tf.concat([[batch_size], [84, 84, 1]], axis=0))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(dropout_rate=0.3):\n",
    "    # Label embeddings\n",
    "    label_embedding_1 = layers.Embedding(10, 16, name=\"label_embedding_1\")\n",
    "    label_embedding_2 = layers.Embedding(10, 16, name=\"label_embedding_2\")\n",
    "    label_embedding_3 = layers.Embedding(10, 16, name=\"label_embedding_3\")\n",
    "    \n",
    "    # Label projections with controlled output size\n",
    "    label_proj_blocks = [\n",
    "        tf.keras.Sequential([\n",
    "            layers.Dense(64, name=f\"label_proj_dense_{i}\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2)\n",
    "        ], name=f\"label_proj_{i}\") for i in range(1, 4)\n",
    "    ]\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    conv_blocks = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv2D(128, kernel_size=3, strides=2, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Flatten()\n",
    "    ], name=\"conv_blocks\")\n",
    "    \n",
    "    # Output blocks\n",
    "    adversarial = tf.keras.Sequential([\n",
    "        layers.Dense(256),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ], name=\"adversarial\")\n",
    "    \n",
    "    def discriminator(img, labels):\n",
    "        # Get batch size from input image\n",
    "        batch_size = tf.shape(img)[0]\n",
    "        \n",
    "        # Ensure labels match batch size\n",
    "        labels = tf.slice(labels, [0, 0], [batch_size, -1])\n",
    "        \n",
    "        # Process each label embedding\n",
    "        emb1 = label_embedding_1(labels[:, 0])\n",
    "        emb2 = label_embedding_2(labels[:, 1])\n",
    "        emb3 = label_embedding_3(labels[:, 2])\n",
    "        \n",
    "        # Process embeddings through projection blocks\n",
    "        feat1 = label_proj_blocks[0](emb1)\n",
    "        feat2 = label_proj_blocks[1](emb2)\n",
    "        feat3 = label_proj_blocks[2](emb3)\n",
    "        \n",
    "        # Combine label features\n",
    "        label_feat = tf.concat([feat1, feat2, feat3], axis=1)\n",
    "        \n",
    "        # Process image through conv blocks\n",
    "        conv_feat = conv_blocks(img)\n",
    "        \n",
    "        # Ensure label_feat has the same batch size as conv_feat\n",
    "        label_feat = tf.broadcast_to(label_feat, [batch_size, tf.shape(label_feat)[1]])\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = tf.concat([conv_feat, label_feat], axis=1)\n",
    "        \n",
    "        # Generate validity score\n",
    "        validity = adversarial(combined_features)\n",
    "        \n",
    "        return validity\n",
    "    \n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator, latent_dim):\n",
    "    # Create the input tensors for the GAN\n",
    "    noise_input = layers.Input(shape=(latent_dim,), name=\"noise_input\")\n",
    "    label_input = layers.Input(shape=(3,), dtype=tf.int32, name=\"label_input\")\n",
    "    \n",
    "    # Generate fake images using the generator\n",
    "    fake_images = generator(noise_input, label_input)\n",
    "    \n",
    "    # Get validity from discriminator\n",
    "    validity = discriminator(fake_images, label_input)\n",
    "    \n",
    "    # Create the GAN model\n",
    "    gan = tf.keras.Model(inputs=[noise_input, label_input], outputs=validity, name=\"gan_model\")\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_visualize(generator, latent_dim):\n",
    "    noise = np.random.normal(0, 1, (3, latent_dim))  # Generate 3 random noise vectors\n",
    "    labels = np.random.randint(0, 10, (3, 3))  # Random labels for the generated images\n",
    "    generated_images = generator.predict([noise, labels])\n",
    "\n",
    "    # Rescale images to [0, 255] for display\n",
    "    generated_images = (generated_images + 1) * 127.5\n",
    "    generated_images = np.clip(generated_images, 0, 255).astype(np.uint8)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))  # 1 row, 3 columns\n",
    "    for i in range(3):\n",
    "        axs[i].imshow(generated_images[i, :, :, 0], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, gan, train_images, train_labels, latent_dim, epochs, batch_size, display_interval=25):\n",
    "    num_samples = train_images.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Generate batch indices\n",
    "        idx = np.random.randint(0, num_samples, batch_size)\n",
    "        \n",
    "\n",
    "        # Get real images and labels\n",
    "        real_images = tf.gather(train_images, idx)\n",
    "        real_labels = tf.gather(train_labels, idx)\n",
    "        \n",
    "        # Generate noise\n",
    "        noise = tf.random.normal((batch_size, latent_dim))\n",
    "        \n",
    "        # Generate fake images\n",
    "        fake_images = generator([noise, real_labels], training=True)\n",
    "        \n",
    "        # Create labels for training\n",
    "        real_labels_ = tf.ones((batch_size, 1))\n",
    "        fake_labels_ = tf.zeros((batch_size, 1))\n",
    "        \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Real images\n",
    "            real_validity = discriminator([real_images, real_labels], training=True)\n",
    "            d_loss_real = tf.keras.losses.binary_crossentropy(real_labels_, real_validity)\n",
    "            \n",
    "            # Fake images\n",
    "            fake_validity = discriminator([fake_images, real_labels], training=True)\n",
    "            d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels_, fake_validity)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = tf.reduce_mean(d_loss_real + d_loss_fake)\n",
    "        \n",
    "        # Apply discriminator gradients\n",
    "        d_grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        discriminator.optimizer.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "        \n",
    "        # Train generator\n",
    "        noise = tf.random.normal((batch_size, latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate images\n",
    "            gen_images = generator([noise, real_labels], training=True)\n",
    "            # Get discriminator output\n",
    "            fake_validity = discriminator([gen_images, real_labels], training=True)\n",
    "            # Calculate generator loss\n",
    "            g_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.binary_crossentropy(real_labels_, fake_validity)\n",
    "            )\n",
    "        \n",
    "        # Apply generator gradients\n",
    "        g_grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        generator.optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % display_interval == 0:\n",
    "            # print(f\"Epoch {epoch+1}/{epochs} - D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\")\n",
    "            generate_and_visualize(generator, latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250 - D Loss: 1.6579, G Loss: 0.7610\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cy/t37pw6kj0l3_0306h2px9b_00000gn/T/ipykernel_35903/4278085891.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdiscriminator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Train the GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/cy/t37pw6kj0l3_0306h2px9b_00000gn/T/ipykernel_35903/602220541.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(generator, discriminator, gan, train_images, train_labels, latent_dim, epochs, batch_size, display_interval)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_labels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_validity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Apply generator gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mg_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs} - D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1062\u001b[0m               output_gradients))\n\u001b[1;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;31m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m# functions for performance reasons in Eager mode. See _Conv2DGrad.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   return [\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[1;32m     45\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1495\u001b[0m         data_format, \"dilations\", dilations)\n\u001b[1;32m   1496\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_dim = 128  # Size of the noise vector\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "display_interval = 50\n",
    "\n",
    "noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "label_input = rs_t_train_labels\n",
    "image_input = f_train_imgs\n",
    "\n",
    "input_noise = tf.keras.Input(shape=(latent_dim,), name=\"noise_input\")\n",
    "input_labels = tf.keras.Input(shape=(3,), name=\"label_input\")\n",
    "input_imgs = tf.keras.Input(shape=(84, 84, 1), name=\"image_input\")\n",
    "\n",
    "\n",
    "# Define the generator and discriminator\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Build the GAN\n",
    "gan = build_gan(generator, discriminator, latent_dim)\n",
    "\n",
    "# Compile the GAN\n",
    "gan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "\n",
    "fake_imgs = generator(input_noise, label_input)\n",
    "validity = discriminator(input_imgs, label_input)\n",
    "\n",
    "# Generate the model by calling the function\n",
    "generator_model = tf.keras.Model(inputs=[input_noise, input_labels], outputs=fake_imgs, name=\"generator\")\n",
    "discriminator_model = tf.keras.Model(inputs=[input_imgs, input_labels], outputs=[validity], name=\"discriminator\")\n",
    "\n",
    "generator_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "discriminator_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator_model, discriminator_model, gan, image_input, label_input, latent_dim, epochs, batch_size, display_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
